{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0622 09:15:15.492971 139958080591680 deprecation_wrapper.py:119] From ../few_shot/dataset/__init__.py:65: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import few_shot.experiments.fashion.config as config\n",
    "from few_shot.experiments.fashion import evaluate_fashion_few_shot\n",
    "from few_shot.dataset.fashion import fashion_dfs\n",
    "from few_shot.dataset.image_pipeline import augmented_img_pipeline_fn, class_augmentation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6044: expected 10 fields, saw 11\\nSkipping line 6569: expected 10 fields, saw 11\\nSkipping line 7399: expected 10 fields, saw 11\\nSkipping line 7939: expected 10 fields, saw 11\\nSkipping line 9026: expected 10 fields, saw 11\\nSkipping line 10264: expected 10 fields, saw 11\\nSkipping line 10427: expected 10 fields, saw 11\\nSkipping line 10905: expected 10 fields, saw 11\\nSkipping line 11373: expected 10 fields, saw 11\\nSkipping line 11945: expected 10 fields, saw 11\\nSkipping line 14112: expected 10 fields, saw 11\\nSkipping line 14532: expected 10 fields, saw 11\\nSkipping line 15076: expected 10 fields, saw 12\\nSkipping line 29906: expected 10 fields, saw 11\\nSkipping line 31625: expected 10 fields, saw 11\\nSkipping line 33020: expected 10 fields, saw 11\\nSkipping line 35748: expected 10 fields, saw 11\\nSkipping line 35962: expected 10 fields, saw 11\\nSkipping line 37770: expected 10 fields, saw 11\\nSkipping line 38105: expected 10 fields, saw 11\\nSkipping line 38275: expected 10 fields, saw 11\\nSkipping line 38404: expected 10 fields, saw 12\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "tf.random.set_random_seed(29)\n",
    "train_df, val_df, test_df = fashion_dfs(dataset_path='../datasets/fashion-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = 5\n",
    "k_way_test = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras ResNet50 model requires images to be preprocessed in a particular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_class_augm(k):\n",
    "    augmented_batch_fn = class_augmentation_fn(k)\n",
    "    \n",
    "    def preprocess_imagenet(input_tuple, qy):\n",
    "        sx, sy, qx = input_tuple\n",
    "        \n",
    "        return (\n",
    "            (\n",
    "                tf.keras.applications.resnet50.preprocess_input(sx),\n",
    "                sy,\n",
    "                tf.keras.applications.resnet50.preprocess_input(qx)\n",
    "            ),\n",
    "            qy\n",
    "        )\n",
    "    \n",
    "    return preprocess_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = config.IMG_SHAPE\n",
    "emb_input = tf.keras.layers.Input(shape=img_shape)\n",
    "\n",
    "resnet_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=img_shape,\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# start by only training the last embedding block\n",
    "for layer in resnet_model.layers[:-32]:  # 11 for conv block, 10 * 2 for the identity blocks, 1 for pooling layer\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in resnet_model.layers[-32:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet_model(emb_input)\n",
    "\n",
    "# protonet expects a dense layer as its input\n",
    "flattened_resnet = tf.keras.layers.Flatten()(resnet)\n",
    "\n",
    "embedding_model = tf.keras.models.Model(inputs=emb_input, outputs=flattened_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 09:15:25.372928 139958080591680 deprecation.py:323] From /home/andre/Projects/fashion_few_shot/.venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0622 09:15:25.414747 139958080591680 deprecation_wrapper.py:119] From ../few_shot/dataset/image_pipeline.py:55: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\n",
      "\n",
      "W0622 09:15:25.475375 139958080591680 deprecation_wrapper.py:119] From ../few_shot/dataset/image_pipeline.py:59: The name tf.image.resize_image_with_pad is deprecated. Please use tf.compat.v1.image.resize_image_with_pad instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fashion experiment 5-shot, 15 way\n",
      "{'augment': True, 'post_processing_fn': <function preprocess_class_augm.<locals>.preprocess_imagenet at 0x7f49f3e7dd08>, 'validation_metric': 'loss', 'reduction_factor': 0.75, 'reduce_lr_on_plateau': False, 'embedding_fn': <function <lambda> at 0x7f49f3e7dae8>, 'restore_best_weights': True, 'patience': 5, 'img_shape': (80, 60, 3), 'test_eps': 1000, 'n_epochs': 100, 'k_way_train': 30, 'n_queries_test': 1, 'n_queries_train': 5, 'lr': 0.001, 'k_way_test': 15, 'n_shot': 5, 'eps_per_epoch': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 09:15:25.746461 139958080591680 deprecation.py:323] From ../few_shot/dataset/__init__.py:101: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0622 09:15:35.592292 139958080591680 deprecation_wrapper.py:119] From ../few_shot/model.py:69: The name tf.linalg.transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
      "\n",
      "W0622 09:15:35.595493 139958080591680 deprecation_wrapper.py:119] From ../few_shot/model.py:70: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "100/100 [==============================] - 334s 3s/step - loss: 7.4726 - categorical_accuracy: 0.6955\n",
      "{'loss': [7.472556512951851], 'categorical_accuracy': [0.69553334], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 137s 1s/step - loss: 1.6613 - categorical_accuracy: 0.5153\n",
      "epoch 0: val_loss: 1.6612762129306793, val_cat_accuracy: 0.515333354473114\n",
      "Training:\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 280s 3s/step - loss: 0.4581 - categorical_accuracy: 0.8468\n",
      "{'loss': [0.45805410817265513], 'categorical_accuracy': [0.8468], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 132s 1s/step - loss: 1.6767 - categorical_accuracy: 0.5367\n",
      "epoch 1: val_loss: 1.6767052817344665, val_cat_accuracy: 0.5366666913032532\n",
      "Training:\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 301s 3s/step - loss: 0.3066 - categorical_accuracy: 0.8937\n",
      "{'loss': [0.30662144027650357], 'categorical_accuracy': [0.8936667], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 143s 1s/step - loss: 1.7314 - categorical_accuracy: 0.5473\n",
      "epoch 2: val_loss: 1.7314445054531098, val_cat_accuracy: 0.5473333597183228\n",
      "Training:\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/4\n",
      "100/100 [==============================] - 321s 3s/step - loss: 0.2225 - categorical_accuracy: 0.9217\n",
      "{'loss': [0.2224586621671915], 'categorical_accuracy': [0.9216667], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 142s 1s/step - loss: 1.7749 - categorical_accuracy: 0.5220\n",
      "epoch 3: val_loss: 1.7748516607284546, val_cat_accuracy: 0.5220000147819519\n",
      "Training:\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 314s 3s/step - loss: 0.1783 - categorical_accuracy: 0.9345\n",
      "{'loss': [0.17833669252693654], 'categorical_accuracy': [0.93446666], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 139s 1s/step - loss: 1.7486 - categorical_accuracy: 0.5220\n",
      "epoch 4: val_loss: 1.7486003851890564, val_cat_accuracy: 0.5220000147819519\n",
      "Training:\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 6/6\n",
      "100/100 [==============================] - 325s 3s/step - loss: 0.1561 - categorical_accuracy: 0.9439\n",
      "{'loss': [0.1561004527192563], 'categorical_accuracy': [0.9439333], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 142s 1s/step - loss: 1.7672 - categorical_accuracy: 0.5033\n",
      "epoch 5: val_loss: 1.7671974313259124, val_cat_accuracy: 0.503333330154419\n",
      "Training:\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 7/7\n",
      "100/100 [==============================] - 325s 3s/step - loss: 0.1402 - categorical_accuracy: 0.9488\n",
      "{'loss': [0.14023890729993582], 'categorical_accuracy': [0.9488], 'lr': [0.001]}\n",
      "Validation:\n",
      "100/100 [==============================] - 141s 1s/step - loss: 1.7762 - categorical_accuracy: 0.5093\n",
      "epoch 6: val_loss: 1.7761772608757018, val_cat_accuracy: 0.5093333125114441\n",
      "1000/1000 [==============================] - 1215s 1s/step - loss: 2.0461 - categorical_accuracy: 0.4372\n"
     ]
    }
   ],
   "source": [
    "print(f'Running fashion experiment {n_shots}-shot, {k_way_test} way')\n",
    "assert k_way_test <= test_df.class_name.nunique()\n",
    "result = evaluate_fashion_few_shot(train_df=train_df,\n",
    "                                   val_df=val_df,\n",
    "                                   test_df=test_df,\n",
    "                                   n_shot=n_shots,\n",
    "                                   k_way_test=k_way_test,\n",
    "                                   patience=5,\n",
    "                                   restore_best_weights=True,\n",
    "                                   augment=True,\n",
    "                                   post_processing_fn=preprocess_class_augm(config.K_WAY_TRAIN),\n",
    "                                   embedding_fn=lambda x: embedding_model\n",
    "                                  \n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion_few_shot",
   "language": "python",
   "name": "fashion_few_shot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
