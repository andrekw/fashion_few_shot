* Few-Shot learning for fashion

In this repository I will experiment with applying few-shot learning to the [[https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset/version/1][Kaggle fashion dataset]] using TensorFlow and Keras.

** Setup

You will need Python >= 3.6, pipenv and preferably access to a GPU. 

The dataset should be unzipped to ~datasets/fashion-dataset~, i.e., after extraction you should have a ~datasets/fashion-dataset/images~ folder and a ~datasets/fashion-dataset/styles.csv~ file.
I've had corruption errors when uncompressing the dataset on CentOS.
It works fine on a mac, however.

** Prototypical Networks

This implementation of [[https://arxiv.org/abs/1703.05175][Prototypical Networks for Few-shot Learning]] by Snell, Swersky and Zemel is heavily inspired by [[https://github.com/oscarknagg/few-shot/][Oscar Knagg's implementation]].

*** Omniglot

As a sanity check for my implementation, I've ran a 5-way, 1-shot experiment on [[https://github.com/brendenlake/omniglot/blob/master/python/images_evaluation.zip][Omniglot]] and managed to replicate the results. Follow these steps to run this experiment:
- First download the dataset and process it with Oscar's ~prepare_omniglot.py~.
- Place the processed dataset under ~datasets/Omniglot~.
- Call ~pipenv run python -m few_shot.experiments.omniglot~.

This takes around 20 minutes to train and evaluate on a GeForce 960.

*** Reference model

My first goal was to implement the same model and training strategy as in the miniImageNet experiment in the Protonets paper. All experiments were evaluated in 1- and 5-shot, and 5- and 15-way episodes. Each episode holds 5 query points per class, meaning we can use all classes with 10 or more samples. I have also experimented with 15 queries per class, but that meant excluding even more classes and samples.

**** Building a dataset

The module ~few_shot.dataset.fashion~ has helper functions for building few-shot episodes from the fashion data. 
~build_fashion_df~ loads the csv files and finds all rows with corresponding images.
~TRAINING_CLASSES~ and ~TEST_CLASSES~ contain a random split of all classes with more than 10 examples for training and testing.
The convenience function ~fashion_dfs~ builds train, validation and test sets. The validation set is composed of 16 random classes from the training set.
~few_shot.dataset.FewShotEpisodeGenerator~ takes one of these DataFrames, handles loading and processing images, and yields batches of episodes.

**** Image pipeline

This dataset contains very high resolution images. Images are resized to a fixed size as part of the data fetching process. 
The module ~few_shot.dataset.image_pipeline~ contains functions handling this part of the process.
There is also a image augmentation pipeline in the same module. It translates, rotates, scales, flips horizontally, changes brightness randomly and adds random noise to the image.

**** Run experiments

The module ~few_shot.experiments.fashion~ contains the experiments I've ran. Each submodule is a different experiment with different settings. ~few_shot.experiments.fashion.config~ contains default settings.

- ~default_params.py~ follows the training procedure from the paper: a 4-convolutional-block model trained with Adam and early stopping;
- ~data_augmentation.py~ adds data augmentation to the procedure above and results in a slight improvement in the 5-shot setting, but is slightly worse in 1-shot;
- ~hyperparameter_search.py~ performs Bayesian Optimization on the model hyperparameters using scikit-opt. It currently supports tuning the optimizer (Adam or RMSprop), the learning rate, the number of convolutional blocks, dropout rate, early stopping patience, and using a normal or increased k for training.

*** To dos

**** Tuning image augmentation

The augmentation might be too strong and the model might have too hard of a time learning (Too much noise, brightness changes, etc). Especially since the images in this set are professionally-taken, variations related to exposure, for example might be actually detrimental.

**** No early stopping

- Early stopping seems to be very aggressive. The paper just mentions they "...train until validation loss stops improving." (p. 6). This seems too aggressive in my implementation, it stops training after only 300-400 episodes. I want to let it run longer and retrieve the best-performing weights after a large number of episodes.

**** Hyperparameter search

- This takes a very long time to run, I haven't had enough time to let it go through enough iterations to make it worthwhile. I'd estimate at least 50 runs at least.

**** Class augmentations

- Try upside-down images as new classes for training;
- Something like [[https://arxiv.org/abs/1706.00409][Fader Networks]] to generate new classes and samples based on combinations of attributes.

**** Other approaches

- Matching networks
- Model-agnostic Meta-Learning
